{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# Détection et mise en correspondance de primitives\n",
    "\n",
    "\n",
    "## 1. Déscription\n",
    "\n",
    "Le but de la détection et de la mise en correspondance de primitives est d'établir le lien entre un point dans une image *X* et son correspondant dans une autre image *Y*. Ces correspondances peuvent ensuite être utilisées pour assembler plusieurs images dans un panorama.\n",
    "\n",
    "Dans ce devoir, vous allez écrire du code pour détecter les primitives discriminantes (qui sont raisonnablement invariantes à la translation, la rotation et le changement dans l'éclairage) dans une image et trouver les meilleures primitives correspondantes dans une autre image. Le devoir comprend trois parties : détection des primitives, description des primitives et mise en correspondance des primitives. \n",
    "\n",
    "Un certain nombre de classes et de méthodes vous sont déjà fournis. Il est primordial de comprendre le rôle de chaque classe et fonction avant d'essayer de modifier le code. Prenez donc le temps de consulter les fichiers sources et les commentaires fournis. \n",
    "\n",
    "**<span style='color:Red'> Le code que vous devez écrire et toutes les modifications requises seront dans le fichier `features.py`</span>.**\n",
    "\n",
    "Pour vous aider à visualiser vos résultats et déboguer votre implémentation, une interface graphique est fournit (fichier **`featuresUI.py`**). Cette interface utilisateur permet d'afficher les primitives détectées et les meilleures correspondances entre deux image. \n",
    "\n",
    "Un exemple de détecteur/descripteur de primitive appelé \"[Oriented FAST and Rotated BRIEF (ORB)](https://medium.com/analytics-vidhya/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf)\" est également fournit pour comparaison. Cette technique est très populaire en Vision par Ordinateur et se propose comme alternative aux algorithmes [SIFT](https://docs.opencv.org/3.4.9/da/df5/tutorial_py_sift_intro.html) et [SURF](https://docs.opencv.org/3.4.9/df/dd2/tutorial_py_surf_intro.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Les cellules de code dans ce blocs-note vous permettent de tester votre \n",
    "implémentation des différentes tâches de ce devoir. Si vos tests échouent, \n",
    "vous devez vérifier pourquoi ils ont échoué. En particulier, faites attention \n",
    "aux tolérances/seuils utilisés dans ce bloc-notes. Il est possible que votre \n",
    "réponse soit correcte, mais elle tombe à peine en dehors de la plage de tolérance.\n",
    "'''\n",
    "# Numpy est le paquetage principal utilisé pour le calcul scientifique dans Python. \n",
    "import numpy as np\n",
    "\n",
    "# cv2 est le paquetage OpenCV pour Python - il est utilisé pour implémenter/utiliser \n",
    "# des fonctionalités en traintement d'image et/ou en vision par ordinateur\n",
    "import cv2\n",
    "\n",
    "# Ce module fournit une interface standard pour extraire, formater et imprimer \n",
    "# les traces de pile des programmes Python\n",
    "import traceback\n",
    "\n",
    "# PIL (Python Image Library) est un paquetage utilisé pour manipuler les images sous Python \n",
    "from PIL import Image\n",
    "\n",
    "# Quelques instructions supplémentaires pour que le notebook recharge les modules externes en python;\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Chargement du fichier de l'étudiant - c.-à-d. le fichier features.py\n",
    "import features\n",
    "\n",
    "# Sauvegarde et chargement de points cv2\n",
    "def pickle_cv2(arr):\n",
    "    index = []\n",
    "    for point in arr:\n",
    "        temp = (point.pt, point.size, point.angle, point.response, point.octave, point.class_id)\n",
    "        index.append(temp)\n",
    "    return np.array(index, dtype=object)\n",
    "\n",
    "def unpickle_cv2(arr):\n",
    "    index = []\n",
    "    for point in arr:\n",
    "        temp = cv2.KeyPoint(x=point[0][0],\n",
    "                            y=point[0][1],\n",
    "                            _size=point[1], \n",
    "                            _angle=point[2], \n",
    "                            _response=point[3], \n",
    "                            _octave=point[4], \n",
    "                            _class_id=point[5])\n",
    "        index.append(temp)\n",
    "    return np.array(index)\n",
    "\n",
    "# Fonctions pour tester l'exactitude de deux tableaux élément-par-élément\n",
    "def compare_array(arr1, arr2):    \n",
    "    return np.allclose(arr1,arr2,rtol=1e-3,atol=1e-5)\n",
    "\n",
    "# Fonction pour comparer deux points cv2\n",
    "def compare_cv2_points(pnt1, pnt2):\n",
    "    if not np.isclose(pnt1.pt[0],pnt2.pt[0],rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.pt[1],pnt2.pt[1],rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.angle,pnt2.angle,rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.response,pnt2.response,rtol=1e-3,atol=1e-5): return False\n",
    "    return True\n",
    "\n",
    "# fonction appelée pour tester les différentes tâches à implémenter\n",
    "def try_this(todo, run, truth, compare, *args, **kargs):\n",
    "    '''\n",
    "    Exécute une fonction, teste le résultat avec 'compare', et affiche un message d'erreur \n",
    "    si ça ne fonctionne pas\n",
    "    @arg todo (int or str): Le numéro du TODO \n",
    "    @arg run (func): La fonction à exécuter\n",
    "    @arg truth (any): Le résultat correct  de la fonction testée\n",
    "    @arg compare (func->bool): retourne un booléen de la comparaison de la sortie de la fonction `run` avec le résultat correct\n",
    "    @arg *args (any): Arguments supplémentaires éventuels à passer à la fonction `run`\n",
    "    @arg **kargs (any): Arguments supplémentaires éventuels à passer à la fonction `compare`\n",
    "\n",
    "    @return (int): Le nombre de tests qui ont échoué\n",
    "    '''\n",
    "    print('Starting test for TODO {}'.format(todo))\n",
    "    failed = 0\n",
    "    try:\n",
    "        output = run(*args)\n",
    "        print(\"ok\")\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"TODO {} threw an exception, see exception below\\n\".format(todo))\n",
    "        return\n",
    "    if type(output) is list or type(output) is tuple:\n",
    "        for i in range(len(output)):\n",
    "            if not compare(output[i], truth[i], **kargs):\n",
    "                print(\"TODO {} doesn't pass test: {}\".format(todo, i))\n",
    "                failed+=1\n",
    "    else:\n",
    "        if not compare(output, truth, **kargs):\n",
    "            print(\"TODO {} doesn't pass test\".format(todo))\n",
    "            failed+=1\n",
    "    return failed\n",
    "\n",
    "\n",
    "image = np.array(Image.open('resources/triangle1.jpg'))\n",
    "grayImage = cv2.cvtColor(image.astype(np.float32)/255.0, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "'''\n",
    "Chargement des tableaux numpy contenant les résultats de triangle1.jpg.\n",
    "\n",
    "Ces tableaux sont accessibles à l'aide de loaded['<lettre>']. \n",
    "Par exemple, la réponse correcte pour le test 2 est 'c', donc pour voir \n",
    "la réponse correcte pour le test 2 vous pouvez inspecter loaded['c'].\n",
    "Remarque importante : NumPy n'affiche pas l'ensemble du tableau s'il est \n",
    "très grand --- vous devez afficher des parties plus petites (e.g., \n",
    "print( repr(loaded['c'][0]) ) ).\n",
    "'''\n",
    "loaded = np.load('resources/arrays.npz', allow_pickle=True)\n",
    "d = unpickle_cv2(loaded['d_proc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Tâches principales\n",
    "\n",
    "### 2.1. Détection de points-clés ( 35 points )\n",
    "\n",
    "Dans cette étape, vous identifierez les points-clés dans l'image à l'aide de la méthode de détection des coins Harris. Pour chaque point de l'image, vous devez réaliser les étapes suivantes (voir les slides du cours pour plus de détails) : \n",
    "\n",
    "1. Définissez une fenêtre de pixels autour de ce point.\n",
    "\n",
    "\n",
    "2. Calculez la matrice de Harris *H* pour (la fenêtre autour de) ce point, définie comme\n",
    " \n",
    "    $\n",
    "     \\begin{align}\n",
    "     H & = \\sum_p w_p \\nabla I_p (\\nabla I_p)^T\n",
    "     \\\\\n",
    "     & = \\sum_p w_p \\begin{bmatrix} \n",
    "                 {I_{x_p}}^2 & I_{x_p} I_{y_p} \\\\ \n",
    "                  I_{x_p} I_{y_p} & {I_{y_p}}^2\n",
    "               \\end{bmatrix}               \n",
    "     \\\\\n",
    "     & = \\sum_p     \\begin{bmatrix} \n",
    "                 w_p {I_{x_p}}^2 & w_p I_{x_p} I_{y_p} \\\\ \n",
    "                 w_p I_{x_p} I_{y_p} & w_p  {I_{y_p}}^2\n",
    "               \\end{bmatrix}               \n",
    "     \\\\\n",
    "     & = \\begin{bmatrix} \n",
    "       \\sum_p w_p {I_{x_p}}^2 & \\sum_p w_p I_{x_p} I_{y_p} \\\\ \n",
    "       \\sum_p w_p I_{x_p} I_{y_p} & \\sum_p w_p  {I_{y_p}}^2\n",
    "     \\end{bmatrix}    \n",
    "     \\end{align}    \n",
    "    $\n",
    "\n",
    "    où la somme est sur tous les pixels *p* de la fenêtre. \n",
    "    \n",
    "    *H* est une matrice $2 \\times 2$. $I_{x_p}$ est la dérivée *x* de l'image au point *p*, la notation est similaire pour la dérivée *y*. Vous devez utiliser l'opérateur de Sobel $3 \\times 3$ pour calculer les dérivées *x* et *y* (utilisez le mode \"réflexion\" pour extrapoler les pixels aux bords de l'image). Les poids $w_p$ doivent être circulairement symétriques (pour l'invariance à la rotation) - utilisez un masque gaussien $5 \\times 5$ avec $\\sigma = 0.5$.<br>\n",
    "\n",
    "\n",
    "3. Utilisez ensuite *H* pour calculer la fonction d'intensité de coin $c(H)$ à chaque pixel. En ce sens, utilisez la définition alternative ci-dessous du score $c(H)$ où *k* est une constante déterminée empiriquement; utilisez la valeur $k = 0.1$.\n",
    " \n",
    "    $    \n",
    "     c(H) = \\lambda_1 \\lambda_2 - k \\cdot (\\lambda_1 + \\lambda_2)^2  = det(H) - k \\cdot (trace(H))^2     \n",
    "    $\n",
    "\n",
    "\n",
    "4. Vous aurez également besoin de l'angle d'orientation à chaque pixel. Utilisez l'angle du gradient comme une approximation. L'angle zéro pointe vers la droite et les angles positifs sont dans le sens de rotation des aiguilles d'une montre. **Remarque : Ne pas calculer l'orientation par la méthode des vecteurs propres**.\n",
    "\n",
    "\n",
    "5. Sélectionnez les points-clés les plus forts et qui correspondent aux maxima locaux de $c(H)$. Utilisez une fenêtre $7 \\times 7$ pour vos calculs.\n",
    "\n",
    "\n",
    "####  Code à écrire\n",
    "\n",
    "La fonction **`detectKeypoints`** dans **` HarrisKeypointDetector`** est l'une des principales fonctions que vous devez compléter avec les fonctions auxilières **`computeHarrisValues`** (calcule le score et l'orientation de Harris pour chaque pixel de l'image) et **`computeLocalMaxima`** (calcule un tableau de booléens qui indique pour chaque pixel s'il est un maximum local ou non). L'ensemble de ces fonctions implémente le détecteur de coins de Harris. \n",
    "\n",
    "Les fonctions suivantes vous seront peut-être utiles pour implémenter cette tâche :\n",
    "\n",
    "- `scipy.ndimage.sobel`: Filtre l'image en entrée avec le filtre Sobel.\n",
    "- `scipy.ndimage.gaussian _filter`: Filtre l'image en entrée avec un filtre gaussien.\n",
    "- `scipy.ndimage.filters.maximum_ filter`: Filtre l'image en entrée avec un filtre maximum.\n",
    "- `scipy.ndimage.filters.convolve`: Filtre l'image en entrée avec un noyau spécifique.\n",
    "\n",
    "Vous pouvez consulter la documentation sur ces fonctions [ici](http://docs.scipy.org/doc/scipy/reference/ndimage.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Nous allons tester ici votre implémentation des TODOs 1, 2 et 3 dans features.py\n",
    "sur l'exemple d'image triangles1.jpg - N'oubliez pas d'éxecuter la cellule précédente\n",
    "de code Python pour initialiser l'environement de test.\n",
    "'''\n",
    "\n",
    "# construction de l'objet HKD pour la détection des points-clés\n",
    "HKD = features.HarrisKeypointDetector()\n",
    "# appel de la fonction de test pour le TO-DO n°1\n",
    "try_this(1, HKD.computeHarrisValues, [loaded['a'],loaded['b']], compare_array, grayImage)\n",
    "\n",
    "# patcher HKD afin que les futurs tests n'échouent pas si le dernier test a échoué\n",
    "class HKD2(features.HarrisKeypointDetector):\n",
    "  def computeHarrisValues(self,image):\n",
    "    return loaded['a'],loaded['b']\n",
    "HKD=HKD2()\n",
    "# appel de la fonction de test pour le TO-DO n°2\n",
    "try_this(2, HKD.computeLocalMaxima, loaded['c'], compare_array, loaded['a'])\n",
    "\n",
    "# patcher HKD afin que les futurs tests n'échouent pas si le dernier test a échoué\n",
    "class HKD3(HKD2):\n",
    "  def computeLocalMaxima(self,image):\n",
    "    return loaded['c']\n",
    "HKD=HKD3()\n",
    "# appel de la fonction de test pour le TO-DO n°3\n",
    "try_this(3, HKD.detectKeypoints, d, compare_cv2_points, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Descripteur de primitive  ( 35 points )\n",
    "\n",
    "Une fois les points d'intérêt identifiés, l'étape suivante consiste à trouver un *descripteur* pour chaque point-clé détecté. Ce descripteur sera la représentation que vous utiliserez pour comparer des primitives dans différentes images et voir si elles correspondent. Vous implémenterez deux descripteurs pour ce devoir : \n",
    "\n",
    "- En premier, vous implémenterez un descripteur simple conçu des valeurs d'intensité de pixel dans un voisinage $5 \\times 5$ du point-clé. Cela devrait être facile à mettre en oeuvre et devrait fonctionner correctement quand les images comparées sont liées par une simple translation.\n",
    "\n",
    "\n",
    "- Deuxièmement, vous implémenterez une version simplifiée du descripteur MOPS. En ce sens, implémentez la matrice qui vous permettra de transformer une fenêtre $40 \\times 40$ en un patch de $8 \\times 8$ pixels après l'avoir pivotée d'un angle $\\theta$ autour du point-clé. La nouvelle orientation du point-clé devra être vers la droite. \n",
    "\n",
    "  Vous devez également normaliser les intensités du patch pour avoir une moyenne nulle et une variance égale à 1. Si la variance du patch avant normalisation est très proche de zéro (inférieure à $10^{-10}$ en valeur absolue), renvoyez simplement un descripteur entièrement nul pour éviter une erreur de division par zéro.\n",
    "  \n",
    "  Utilisez `cv2.warpAffine` de OpenCV pour effectuer la transformation. La fonction `warpAffine` requiert une matrice $2 \\times 3$ en entrée. La façon la plus simple de générer cette matrice est de combiner plusieurs transformations basiques. Une séquence de translation $\\mathbf{T_1}$, rotation $\\mathbf{R}$, redimensionnement $\\mathbf{S}$ et translation $\\mathbf{T_2}$ produira la matrice escompté. Notez que les transformations sont combinées de droite à gauche, de sorte que la matrice de transformation est le produit matriciel $\\mathbf{T_2\\;S\\;R\\;T_1}$. Les figures ci-dessous illustrent la séquence.\n",
    "  \n",
    "\n",
    "![](warpAffine.png)\n",
    "\n",
    "\n",
    "#### Code à écrire\n",
    "\n",
    "Vous devez implémenter deux descripteurs de primitives dans les classes **`SimpleFeatureDescriptor`** et **` MOPSFeaturesDescriptor`**. La méthode **`describeFeatures`** de ces classes prend en entrée les informations de position et d'orientation des points-clés déjà calculés (par exemple par le détecteur Harris) et calcule les descripteurs pour ces points-clés. Ces descripteurs sont ensuite stockés dans un tableau Numpy bidimensionnel. Le nombre de lignes de ce tableau est égale au nombre de point-clés détectés. Le nombre de colonnes du tableau est égale à la dimension du descripteur retourné (par exemple, 25 pour le descripteur simple de primitives $5 \\times 5$).\n",
    "\n",
    "Pour l'implémentation de MOPS, vous devez créer une matrice pour transformer une fenêtre $40 \\times 40$ en un patch de $8 \\times 8$ pixels après l'avoir pivotée d'un angle $\\theta$ autour du point-clé (comme décrit ci-dessus). \n",
    "\n",
    "Il est recommandé de consulter la [documentation opencv](https://docs.opencv.org/3.4.9/d4/d61/tutorial_warp_affine.html) sur **`cv2.warpAffine`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Nous allons tester ici votre implémentation des TODOs 4, 5 et 6 dans features.py\n",
    "sur l'exemple d'image triangles1.jpg\n",
    "'''\n",
    "\n",
    "# construction de l'objet SFD pour un descripteur simple d'intensités\n",
    "SFD = features.SimpleFeatureDescriptor()\n",
    "\n",
    "# appel de la fonction de test pour le TO-DO n°4\n",
    "try_this(4, SFD.describeFeatures, loaded['e'], compare_array, image, d)\n",
    "\n",
    "# construction de l'objet MFD pour un descripteur MOPS\n",
    "MFD = features.MOPSFeatureDescriptor()\n",
    "\n",
    "# appel de la fonction de test pour le TO-DO n°5 et/ou n°6\n",
    "try_this('5 and/or 6', MFD.describeFeatures, loaded['f'], compare_array, image, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Mise en correspondance de primitives ( 30 points )\n",
    "\n",
    "Une fois les points-clés détectés et leurs descripteurs calculés pour un ensemble d'images, l'étape suivante consiste à établir les correspondances entre les différents points-clés identifiés dans les images (c.-à-d., étant donné une primitive dans une image *X*, trouver la meilleure primitive correspondante dans une autre image *Y*).\n",
    "\n",
    "\n",
    "L'approche la plus simple est la suivante : Pour chaque descripteur d'une image X, le comparer aux descripteurs d'une image *Y* en calculant une *distance* scalaire. La meilleure correspondance est la paire de primitives qui produit la plus petite distance. Vous implémenterez deux fonctions de distance :\n",
    "\n",
    "1. Somme des moindres carrés (SMC): il s'agit de la distance euclidienne au carré entre les deux descripteurs.\n",
    "\n",
    "\n",
    "2. Ratio de distances : le rapport des distances SMC associées aux deux meilleurs descripteurs identifiés (c.-à-d. le ratio est égale à la distance SMC du descripteur le plus proche divisée par la distance SMC du second descripteur le plus proche).\n",
    "\n",
    "#### Code à écrire\n",
    "\n",
    "Dans cette tache, vous allez implémenter une fonction pour la mise en correspondance de primitives. Vous implémenterez la fonction **`matchFeatures`** de **` SSDFeatureMatcher`** et de **`RatioFeatureMatcher`**. Ces fonctions renvoient une liste d'objets [cv2.DMatch](https://docs.opencv.org/3.4.9/d4/de0/classcv_1_1DMatch.html). \n",
    "\n",
    "Vous devez initialiser l'attribut `queryIdx` de `cv2.DMatch` à l'index de la primitive dans la première image, l'attribut` trainIdx` à l'index de la primitive dans la deuxième image et l'attribut `distance` à la distance (ou le ratio, selon le cas) entre les deux primitives. \n",
    "\n",
    "Les fonctions `scipy.spatial.distance.cdist` et` numpy.argmin` pourrait vous être utiles lors de l'implémentation de cette tache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "En vous inspirant des tests précédents, vous pouvez implémenter ici vos tests\n",
    "pour les TODOs 7 et 8 dans features.py\n",
    "'''\n",
    "\n",
    "SSDFM = features.SSDFeatureMatcher()\n",
    "\n",
    "# TODO 7: tester la fonction de mise en correspondance 'matchFeatures' de la classe 'SSDFeatureMatcher' \n",
    "# TODO 8: tester la fonction de mise en correspondance 'matchFeatures' de la classe 'RatioFeatureMatcher'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tâche bonus (20 points)\n",
    "\n",
    "Des points supplémentaires sont accordés pour les solutions qui améliorent, d'au moins 15%, l'**A**ire **S**ous la **C**ourbe (ASC) moyenne pour le test utilisant le \"ratio de distances\". Une amélioration de 15% est interprétée comme une réduction de 15% de la valeur $(1 - ASC)$. C'est-à-dire la zone au-dessus de la courbe. Voici une suggestion (et nous vous encourageons également à proposer vos propres idées!)\n",
    "\n",
    " - Implémentez la méthode **`selectKeypointsANMS`** de la classe **`KeypointDetector`**. Cette méthode devrait réaliser la technique de suppression non maximale adaptative discutée dans l'[article MOPS](https://drive.google.com/open?id=1vly0rexasm-kM_lWEulJ-PiBfCDm8cdO)\n",
    "  \n",
    "Vous pouvez mesurer votre ASC moyenne en exécutant le benchmark de l'interface utilisateur **`featuresUI.py`** sur les cinq ensembles de données (bikes, graf, leuven, wall, yosemite). Nous évaluerons spécifiquement les performances de votre implémentation contre l'ensemble de données 'yosemite'. Les implémentations qui n'améliorent pas l'ASC moyenne d'au moins 15% ne seront pas notées.\n",
    "\n",
    "Le bonus sera accordé en fonction du mérite de votre amélioration et de **sa justification**. Les modifications hyperparamétriques simples recevront moins de points supplémentaires que les améliorations significatives de l'algorithme.\n",
    "\n",
    "Décrivez votre implémentation dans un fichier nommé **`Lisez-moi_adaptative.txt`** qui décrit vos modifications. \n",
    "\n",
    "\n",
    "## 4. Visualisation\n",
    "\n",
    "L'interface utilisateur dans **`featuresUI.py`** est un outil pour vous aider à visualiser les détections de points-clés et les résultats de correspondance des primitives. Les fonctions de détection de points-clés et les méthodes de mise en correspondance sont appelées à partir de cette interface. N'hésitez pas à étendre les fonctionnalités de cette interface, mais n'oubliez pas que seul le code dans **`features.py`** sera noté.\n",
    "\n",
    "En exécutant **`featuresUI.py`**, vous verrez une interface utilisateur dans laquelle vous avez les choix suivants :\n",
    "\n",
    "- **Détection des points-clés** : <br>\n",
    "  Vous pouvez charger une image et calculer les points d'intérêt avec leurs orientations.\n",
    "\n",
    "\n",
    "- **Mise en correspondance des primitives** : <br>\n",
    "  Ici, vous pouvez charger deux images et afficher les meilleures correspondances calculées à l'aide des algorithmes spécifiés.\n",
    "\n",
    "\n",
    "- **Analyse** : <br>\n",
    "  Après avoir spécifié le chemin d'accès au répertoire contenant l'ensemble de données, le programme exécutera les algorithmes spécifiés sur toutes les images et calculera les courbes ROC pour chaque image.\n",
    "\n",
    "\n",
    "Egalement founit est un ensemble d'images de référence pour tester les performances de votre algorithme en fonction de différents types de variation contrôlée (c'est-à-dire, rotation, redimensionnement, éclairage, perspective, flou). Pour chacune de ces images, nous connaissons la transformation correcte et pouvons donc mesurer la précision de chacune de vos détection et/ou mise en correspondance de primitives. Gardez à l'esprit que votre code sera évalué numériquement, pas visuellement.\n",
    "\n",
    "Vous devriez également sortir et prendre vos propres photos pour voir à quel point votre approche fonctionne sur des ensembles de données plus intéressants.\n",
    "\n",
    "\n",
    "## 5. Quoi remettre\n",
    "\n",
    "### 5.1 Règles de codage à respecter\n",
    "\n",
    "- Vous pouvez utiliser les fonctions NumPy, SciPy et OpenCV2 pour implémenter des opérations mathématiques, de filtrage et de transformation. N'utilisez pas de fonctions qui implémentent la détection de points-clés ou la mise en correspondance de primitives - ça sera un zéro pour tout le devoir si vous le faites !\n",
    "\n",
    "\n",
    "- Lorsque vous utilisez l'opérateur de Sobel ou le filtre gaussien, vous devez utiliser le mode « réflexion » pour avoir un gradient nul sur les bords.\n",
    "\n",
    "\n",
    "- Voici une liste de fonctions potentiellement utiles (vous n'êtes pas obligé de les utiliser) :\n",
    "\n",
    "      - scipy.ndimage.sobel\n",
    "      - scipy.ndimage.gaussian_filter\n",
    "      - scipy.ndimage.filters.convolve\n",
    "      - scipy.ndimage.filters.maximum_filter\n",
    "      - scipy.spatial.distance.cdist\n",
    "      - cv2.warpAffine\n",
    "      - np.max, np.min, np.std, np.mean, np.argmin, np.argpartition\n",
    "      - np.degrees, np.radians, np.arctan2\n",
    "    \n",
    "\n",
    "### 5.2. Le code (à remettre en classe)\n",
    "\n",
    "Un fichier zip contenant le fichier **`features.py`** et eventuellement un fichier **`Lisez-moi_adaptative.txt`** si vous avez implémenté la tache bonus\n",
    "\n",
    "**Le code sera remis en classe pendant votre séance de TP au serveur INGInious - <span style='color:Red'> aucun document ou code ne sera accepté si envoyé par ~~\"Google Classroom\",~~ mail ou présenté sur clé USB</span>**.\n",
    "\n",
    "### ~~5.3. Le rapport (à remettre sur Google Classroom)  (15 points)~~\n",
    " \n",
    "~~Editez un court rapport qui décrit **clairement** votre travail en incluant des résultats d'analyse comparative (benchmark) en termes de courbes ROC et ASC sur le **jeu de données Yosemite** fourni dans le répertoire `resources`. Vous pouvez obtenir des courbes ROC.~~\n",
    "\n",
    "1. ~~en exécutant **`featuresUI.py`**,~~ \n",
    "2. ~~en basculant vers l'onglet \"Benchmark\",~~ \n",
    "3. ~~en appuyant sur \"Run Benchmark\",~~ \n",
    "4. ~~en sélectionnant le répertoire \"resources/yosemite\",~~ \n",
    "5. ~~et en attendant un court instant.~~ \n",
    "\n",
    "~~Ensuite, l'ASC sera affichée en bas de l'écran (AUC en anglais) et vous pouvez enregistrer la courbe ROC en appuyant sur \"Screenshot\". Vous devez également inclure une image du détecteur Harris. Elle est enregistrée sous le nom `harris.png` chaque fois que les points-clés Harris sont calculés.~~\n",
    "\n",
    "\n",
    "~~Le rapport doit contenir les éléments suivants :~~\n",
    "\n",
    "- ~~Exécutez le benchmark dans **`featuresUI.py`** sur le jeu de données Yosemite pour les quatre configurations possibles impliquant des descripteurs simples ou MOPS avec SMC ou \"ratio de distances\". Incluez les courbes ROC résultantes, signalez les ASC obtenues, et commentez quelle méthode est la meilleure.~~\n",
    "\n",
    "\n",
    "- ~~Inclure l'image Harris pour \"yosemite/yosemite1.jpg\". Commentez sur les caractéristiques des régions mises en évidence dans l'image (c.-à-d. les régions où il y a des points-clés détectés). Y a-t-il des régions de l'image qui auraient dû être mises en évidence mais ne le sont pas ?~~\n",
    "\n",
    "\n",
    "- ~~Incluez une capture d'écran d'une paire de vos propres images et montrez visuellement les performances de mise en correspondance des primitives pour MOPS + \"Ratio de distances\".~~\n",
    "\n",
    "\n",
    "- ~~Si vous avez réalisé la tache bonus, incluez les courbes ROC et indiquez les ASC de l'ensemble de données Yosemite avec votre algorithme personnalisé en utilisant le \"ratio de distances\". Décrivez vos modifications apportées à l'algorithme et pourquoi elles améliorent les performances.~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
