{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "# Devoir 3 &mdash; Images Panoramiques\n",
    "\n",
    "\n",
    "## 1. Initialisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Setup\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.feature import ORB, match_descriptors\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 12.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## 2. Description\n",
    "\n",
    "Ce devoir traite la transformation de l'image, la méthode robuste RANSAC, la projection bilineaire inverse et la fusion d'images dans un panorama.\n",
    "\n",
    "L'assemblage d'images en panorama est un très grand succès de la vision par ordinateur. Matthew Brown et David G. Lowe ont publié un célèbre [article](https://drive.google.com/file/d/1qB54hR4TS_7But2KKkvuk6jTV1uWg8Tf/view?usp=sharing) sur cette technique en 2007. Depuis lors, cette technologie a été largement adoptée dans de nombreuses applications telles que \"**Google Street View**\", les photos panoramiques sur les smartphones et les logiciels d'assemblage tels que **Photosynth** et **AutoStitch**. \n",
    "\n",
    "Dans ce devoir, vous allez mettre en oeuvre un système pour combiner une série d'images se chevauchant en une seule image panoramique. Les fonctions ORB de OpenCV sont utilisées pour la détection des points-clés dans les images et fournissent un appariement initial.\n",
    "\n",
    "Dans un premier temps, vous allez implémenter le code qui calcule la matrice de transformation entre deux images à partir des paires de correnspondance de points-clés dans ces images. Ensuite, et à l'aide de RANSAC, vous allez séparer les bonnes correspondances (inliers) des mauvaises (outliers) afin d'aligner automatiquement les images et déterminer leurs chevauchements ainsi que leurs positions relatives les unes par rapport aux autres. \n",
    "L'étape suivante, pour chaque image à transformer, utilisez la transformation retournée par RANSAC pour implémenter la projections bilineaire inverse dans l'image destination.\n",
    "Enfin, fusionnerez les images résultantes en un seul panorama homogène. \n",
    "\n",
    "Les étapes requises pour créer un panorama sont indiquées ci-dessous. Celles que vous devez implémenter sont indiquées en gras : \n",
    "\n",
    "1. Extraire les primitives dans les images\n",
    "2. Mise en correspondance des primitives\n",
    "3. **Calcul de la matrice de transformation**\n",
    "4. **Alignement des images à l'aide de RANSAC**\n",
    "5. **Projection et interpolation des images**\n",
    "4. **Fusion des images dans un panorama**\n",
    "\n",
    "Pour vous aider à démarrer dans ce devoir, ce bloc-notes contient des tests et/ou affiche les résultats attendus pour l'exemple d'images utilisées.\n",
    "\n",
    "## 3. Règles de codage\n",
    "\n",
    "**<span style='color:Red'>\n",
    "NE MODIFIEZ PAS LE CODE SQUELETTE EN DEHORS DES BLOCS TODO.<br>L'EVALUATEUR AUTOMATIQUE SERA TRES MECHANT AVEC VOUS SI VOUS LE FAITES !\n",
    "</span>**\n",
    "\n",
    "### 3.1. Résumé des fonctions potentiellement utiles (vous n'êtes pas obligé de les utiliser)  \n",
    "- np.divide, \n",
    "- np.eye, \n",
    "- np.ndarray, \n",
    "- np.dot, \n",
    "- np.linalg.svd, \n",
    "- np.linalg.inv\n",
    "\n",
    "### 3.2. Résumé des fonctions <span style='color:Red'>interdites</span>\n",
    "- cv2.findHomography,\n",
    "- cv2.perspectiveTransform,\n",
    "- cv2.warpPerspective,\n",
    "- cv2.remap,\n",
    "- cv2.getAffineTransform,\n",
    "- cv2.getPerspectiveTransform,\n",
    "- cv2.invertAffineTransform,\n",
    "- cv2.warpAffine,\n",
    "- skimage.transform.ProjectiveTransform,\n",
    "- skimage.measure.ransac,\n",
    "- skimage.transform.SimilarityTransform,\n",
    "- skimage.transform.AffineTransform,\n",
    "- skimage.transform.FundamentalMatrixTransform,\n",
    "- skimage.transform.warp,\n",
    "- skimage.transform.warp_coords\n",
    "\n",
    "Vous pouvez utiliser ces fonctions pour le débogage de votre code, mais la version finale ne doit en aucun cas les inclure faute d'avoir un zéro pour le devoir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commençant par charger deux images\n",
    "img1 = imread('resources/uttower/uttower1.jpg')\n",
    "img2 = imread('resources/uttower/uttower2.jpg')\n",
    "imgs = [img1, img2]\n",
    "\n",
    "# construisons l'objet orb pour la detection, description \n",
    "# et mise en correnspondance par la méthode ORB\n",
    "orb = ORB(n_keypoints=1000, fast_threshold=0.05)\n",
    "  \n",
    "# détection des points-clés et construction des\n",
    "# descripteurs associés\n",
    "keypoints_list   = []  # keypoints[i] corresponds to imgs[i]\n",
    "descriptors_list = []  # descriptors[i] corresponds to keypoints[i]\n",
    "for img in imgs:    \n",
    "    orb.detect_and_extract(rgb2gray(img))\n",
    "    keypoints = orb.keypoints\n",
    "    descriptors = orb.descriptors\n",
    "\n",
    "    keypoints_list.append(keypoints)\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "# Affichage des points-clés détectés    \n",
    "keypoints1 = keypoints_list[0]\n",
    "keypoints2 = keypoints_list[1]\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(keypoints1[:,1], keypoints1[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(keypoints2[:,1], keypoints2[:,0], marker='x')\n",
    "plt.axis('off')\n",
    "plt.title('Detected Keypoints for Image 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant effectue la mise en correspondance des deux images et affiche l'ensemble des correspondances détectées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_matches\n",
    "\n",
    "matches_list = []  # matches_list[i] décrit la paire de correspondance entre les descripteurs descriptors[i] et descriptors[i+1]\n",
    "for i in range(len(imgs)-1):\n",
    "    matches = match_descriptors(descriptors_list[i], descriptors_list[i+1], cross_check=True)        \n",
    "    matches_list.append(matches)\n",
    "\n",
    "# traçons les correspondances...\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "ax.axis('off')\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "## 5. Les tâches\n",
    "\n",
    "### 5.1. Estimation de la matrice de transformation [TODO 1] (20 points)\n",
    "\n",
    "Nous avons maintenant une liste de points clés correspondants sur les deux images. Nous allons l'utiliser pour trouver une matrice de transformation qui mappe les points de la première image aux coordonnées correspondantes dans la deuxième image. En d'autres termes, si le point $p_1 = [y_1,x_1]$ dans l'image 1 correspond à $p_2=[y_2, x_2]$ dans l'image 2, nous devons trouver une matrice de transformation $H$ telle que\n",
    "\n",
    "$$\n",
    "\\tilde{p_1}{H}^T = \\tilde{p_2},\n",
    "$$\n",
    "\n",
    "où $\\tilde{p_1}$ et $\\tilde{p_2}$ sont des coordonnées homogènes de $p_1$ et $p_2$.\n",
    "\n",
    "Notez qu'il peut être impossible de trouver la transformation $H$ qui mappe chaque point de l'image 1 **exactement** au point correspondant de l'image 2. Cependant, nous pouvons estimer la matrice de transformation avec les moindres carrés ou la décomposition en valeurs singulières. Étant donné $N$ paires de points-clés correspondants, soit $X_1$ et $X_2$ des matrices $N \\times 3$ dont les lignes sont des coordonnées homogènes des points clés correspondants dans l'image 1 et l'image 2 respectivement. Nous pouvons estimer $H$ en résolvant le problème des moindres carrés,\n",
    "\n",
    "$$\n",
    "X_1 {H}^T = X_2\n",
    "$$\n",
    "\n",
    "Implémentez **`fit_transform_matrix`** dans` panorama.py`\n",
    "\n",
    "*- Indication : lisez les documentations sur [np.linalg.lstsq](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html)* et [np.linalg.svd](https://numpy.org/doc/1.18/reference/generated/numpy.linalg.svd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import fit_transform_matrix\n",
    "\n",
    "# test de validité pour fit_transform_matrix\n",
    "\n",
    "# test donnée en entrée \n",
    "a = np.array([[0.0, 0.0], \n",
    "              [0.0, 1.0], \n",
    "              [1.0, 0.0], \n",
    "              [1.0, 1.0]])\n",
    "\n",
    "# test donnée en sortie\n",
    "v = np.sqrt(2)/2\n",
    "b = np.array([[ 0.0, 0.0],\n",
    "              [   v,   v],\n",
    "              [   v,  -v],\n",
    "              [ v+v, 0.0]])\n",
    "\n",
    "# premier test\n",
    "H1 = fit_transform_matrix(a, a)\n",
    "H1 = H1 / H1[2,2]\n",
    "# sortie attendue\n",
    "sol1 = np.eye(3)\n",
    "\n",
    "#second test\n",
    "H2 = fit_transform_matrix(a, b)\n",
    "H2 = H2 / H2[2,2]\n",
    "# sortie attendue\n",
    "sol2 = np.array([[v, -v, 0],\n",
    "                 [v,  v, 0],\n",
    "                 [0,  0, 1]])\n",
    "\n",
    "# comparaison des résultats\n",
    "if np.allclose(sol1, H1, rtol=1e-05, atol=1e-05) and np.allclose(sol2, H2, rtol=1e-05, atol=1e-05):\n",
    "    print('Implémentation correcte ! ')\n",
    "else:    \n",
    "    print('Implémentation uncorrecte !\\nH1 = %s, \\n\\nH2 = %s' % (H1,H2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### 5.2. Alignement des images à l'aide de RANSAC [TODO 2] (20 points)\n",
    "Plutôt que d'utiliser directement toutes nos correspondances de points-clés commen entrées pour la fonction `fit _transform_ matrix`, nous pouvons plutôt utiliser RANSAC ([\"RANdom SAmple Consensus\"](https://en.wikipedia.org/wiki/Random_sample_consensus)) pour sélectionner uniquement les bonnes correspondances (inliers) à utiliser pour calculer la matrice de transformation.\n",
    "\n",
    "Les étapes de RANSAC sont:\n",
    "    1. Sélectionner un ensemble aléatoire de correspondances\n",
    "    2. Caluler la matrice de transformation\n",
    "    3. Trouver les bonnes correspondances (inliers) en utilisant le seuil donné\n",
    "    4. Répéter le processus et conserver le plus grand nombre de bonne correspondances\n",
    "    5. Recalculer l’estimation par les moindres carrés de la matrice de transformation \n",
    "       en utilisant que les bonnes correspondances\n",
    "\n",
    "Implémentez **`ransac`** dans` panorama.py`. Puis, exécutez le code suivant pour obtenir la matrice de transformation robuste $H$ et l'ensemble des bonnes correspondances détectés par RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panorama import ransac\n",
    "\n",
    "# choix de seed  pour pouvoir comparer le résultat retourné par ransac \n",
    "# par rapport à la solution affichée\n",
    "np.random.seed(131)\n",
    "\n",
    "H, robust_matches = ransac(keypoints1, keypoints2, matches, threshold=1)\n",
    "\n",
    "# Visualisation des correspondances robustes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "plot_matches(ax, img1, img2, keypoints1, keypoints2, robust_matches)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('RANSAC_solution.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('RANSAC Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### 5.3. Projection inverse des images et interpolation bilineaire [TODO3] (40 points)\n",
    "\n",
    "Nous pouvons maintenant utiliser la matrice de transformation $H$ calculée à l'aide de RANSAC pour transformer nos images et créer un panorama ! En ce sens, implémentez la fonction **`warp_image`** dans `panorama.py`. Vous devez implémenter la procédure de projection inverse avec une interpolation bilinéaire ! C-à-d. Pour transformer les pixels de l'image source, vous commencerez à partir des coordonnées de l'image destination et vous utiliserez l'interpolation bilinéaire dans l'image source pour calculer les couleurs des pixels de l'image destination. \n",
    "\n",
    "Une fois cela fait, exécutez le code suivant pour l'appliquer à nos images. Les images seront déformées et l'image 1 sera projetée sur l'image 2.\n",
    "\n",
    "*Indications :*\n",
    "- *Lorsque vous manipulez des coordonnées homogènes, n'oubliez pas de normaliser lorsque vous les reconvertissez en coordonnées cartésiennes*. \n",
    "- *Attention aux points en dehors de l'image source lors de la projection inverse. Vous ne voudriez pas les inclure dans vos calculs*. \n",
    "- *Essayez d'abord de travailler sur le code en bouclant sur chaque pixel (approche classique). Plus tard, vous pouvez optimiser votre code en utilisant des instructions et des astuces numpy (numpy.meshgrid)*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from panorama import get_output_space, warp_image\n",
    "\n",
    "transforms = [H , np.eye(3) ]\n",
    "\n",
    "# La fonction get_ output_space est une fonction auxiliaire qui aide \n",
    "# à trouver le cadre englobant une liste d'images transformées\n",
    "output_shape, offset = get_output_space(imgs, transforms)\n",
    "\n",
    "img1_warped, _ = warp_image(imgs[0], transforms[0], output_shape, offset)\n",
    "img2_warped, _ = warp_image(imgs[1], transforms[1], output_shape, offset)\n",
    "\n",
    "# Plot warped images\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1_warped[:,:,:3])\n",
    "plt.title('Image 1 warped')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2_warped[:,:,:3])\n",
    "plt.title('Image 2 warped')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('warp_image_solution.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('warp_image Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "Maintenant que tous les ingrédients sont là, exécutez le code suivant pour fusionner les deux images transformées avec un mélange moyen des alphas pour obtenir un panorma. Votre panorama peut ne pas sembler bon à ce stade, mais nous utiliserons plus tard d'autres techniques pour obtenir un meilleur résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1 = img1_warped[:,:,3]\n",
    "alpha2 = img2_warped[:,:,3]\n",
    "merged =  img1_warped[:,:,:3] + img2_warped[:,:,:3]\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = alpha1 + alpha2\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "merged = merged / np.maximum(overlap[:,:,None], 1)\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('mean_alpha_merge_solution.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('warp_image Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### 5.4. Fusion linéaire des images [TODO 4] (20 points)\n",
    "Suite aux étapes précédentes, vous remarquerez les filons désagréables au milieu de l'image panoramique. Il est possible de lisser ces artefacts et produire une image plus agréable à l'oeil en utilisant une technique très simple appelée \"contour progressif\" (feathering). Actuellement, tous les pixels de la région de chevauchement des images sont pondérés de manière égale (alpha = 0.5). Cependant, comme les pixels aux extrémités de la zone de chevauchement sont très bien complétés par les pixels de l'autre image, nous pouvons faire en sorte qu'ils contribuent moins au panorama final.\n",
    "\n",
    "Le mélange par \"contour progressif\" peut être effectué avec les étapes suivantes :\n",
    "1. Calculer une carte de pondération pour chaque image\n",
    "   -  Pondérer chaque pixel de l'image source proportionnellement à sa distance du bord. \n",
    "      Les pixels au milieur de l'image ont un poid plus important par rapport aux pixels \n",
    "      aux bords de l'image.  \n",
    "2. Appliquer les cartes de pondérations à leurs images correspondantes\n",
    "3. Pour chaque pixel dans l’image finale, diviser la valeur du pixel (c.-à-d. la couleur) \n",
    "   par la sommes des coefficients de pondération des images initiales à cet pixel.\n",
    "\n",
    "<img src=\"feathering.png\" style=\"width:50%\"/> \n",
    "\n",
    "En ce sens, réecrivez la fonction **`warp_image`** dans `panorama.py` pour qu'elle calcule, en plus de la transformation initialement implémentée, la carte de pondération des alphas de l'image transformée selon la technique du contour progressif. Le calcul des coefficients de pondération doit être controlé par un paramètre supplémentaire appelé `method` où :\n",
    " - method = None     -- aucun changement. La fonction retourne des images avec le canal alpha égale à 1.0\n",
    " - method ='hlinear' -- calcul des coefficients de pondération de l'image transformée dans le sens horizontal seulement\n",
    " - method ='vlinear' -- calcul des coefficients de pondération de l'image transformée dans le sens vertical seulement\n",
    " - method ='linear'  -- calcul des coefficients de pondération de l'image transformée dans le sens horizontal et vertical. \n",
    " \n",
    "Exécutez le code suivant pour fusionner les deux images transformées avec la technique du \"contour progressif\". Régalez-vous :)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img1_warped, m1 = warp_image(imgs[0], transforms[0], output_shape, offset, method='linear')\n",
    "img2_warped, m2 = warp_image(imgs[1], transforms[1], output_shape, offset, method='linear')\n",
    "\n",
    "alpha1 = img1_warped[:,:,3]\n",
    "alpha2 = img2_warped[:,:,3]\n",
    "merged = (alpha1[:,:,None] * img1_warped[:,:,:3]) + (alpha2[:,:,None] * img2_warped[:,:,:3])\n",
    "\n",
    "# Track the overlap by adding the masks together\n",
    "overlap = alpha1 + alpha2\n",
    "mask = m1 + m2\n",
    "\n",
    "# Normalize through division by `overlap` - but ensure the minimum is 1\n",
    "merged[mask,:]= merged[mask,:] / overlap[mask,None]\n",
    "\n",
    "plt.imshow(merged)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('blended_solution.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('warp_image with blending Solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "fr"
   },
   "source": [
    "### 5.5. Fusion d'images multiples dans un panorama [TODO  Bonus] (20 points)\n",
    "Implémentez **`stitch_multiple_images`** dans `panorama.py` pour assembler une liste ordonnée d'images.\n",
    "\n",
    "Étant donné une séquence d'images $m$ ($I_1, I_2,...,I_m$), prenez chaque paire d'images voisine et calculez la matrice de transformation qui convertit les points de l'image $I_{i}$ aux points dans l'image $I_{i+1}$. Ensuite, sélectionnez une image de référence $I_{ref}$, qui se trouve au milieu de la chaîne. Nous voulons que notre image panoramique finale soit référencée par rapport à l'image $I_{ref}$.\n",
    "\n",
    "*- Indication :*\n",
    "- Si vous êtes confus, vous pouvez revoir les slides du cours sur les transformations et comment les combinées pour effectuer une transformation globale à partir d'une chaine de transformations élementaires.\n",
    "- L'inverse de la matrice de transformation $H$ a l'effet inverse. Pensez à utiliser la fonction [`numpy.linalg.inv`] (https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) quand c'est nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from panorama import stitch_multiple_images\n",
    "\n",
    "# Load images to be stitched\n",
    "img1 = imread('resources/yosemite/yosemite1.jpg')\n",
    "img2 = imread('resources/yosemite/yosemite2.jpg')\n",
    "img3 = imread('resources/yosemite/yosemite3.jpg')\n",
    "img4 = imread('resources/yosemite/yosemite4.jpg')\n",
    "\n",
    "#img1 = imread('resources/istanbul/0.jpg')\n",
    "#img2 = imread('resources/istanbul/1.jpg')\n",
    "#img3 = imread('resources/istanbul/2.jpg')\n",
    "#img4 = imread('resources/istanbul/3.jpg')\n",
    "\n",
    "imgs = [img1, img2, img3, img4]\n",
    "    \n",
    "keypoints_list   = []  # keypoints[i] corresponds to imgs[i]\n",
    "descriptors_list = []  # descriptors[i] corresponds to keypoints[i]\n",
    "for img in imgs:\n",
    "    orb.detect_and_extract(rgb2gray(img))\n",
    "    keypoints = orb.keypoints\n",
    "    descriptors = orb.descriptors\n",
    "    keypoints_list.append(keypoints)\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "matches_list = []  # matches_list[i] corresponds to matches between\n",
    "                   # descriptors[i] and descriptors[i+1]\n",
    "for i in range(len(imgs)-1):\n",
    "    #matches = match_descriptors(descriptors[i], descriptors[i+1], 0.7)\n",
    "    matches = match_descriptors(descriptors_list[i], descriptors_list[i+1], cross_check=True)\n",
    "    matches_list.append(matches)\n",
    "\n",
    "# Stitch images together\n",
    "panorama = stitch_multiple_images(imgs, keypoints_list, matches_list, imgref=2, blend='linear')\n",
    "\n",
    "plt.imshow(panorama)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(imread('multiple_ref2_solution.png', as_gray=True))\n",
    "plt.axis('off')\n",
    "plt.title('multiple images panorama solution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Livrable\n",
    "\n",
    "Le fichier **`panorama.py`** contenant vos modifications dans les zones indiquées par `#TODO-BLOC-DEBUT` et `#TODO-BLOC-FIN`.\n",
    "\n",
    "**Le code sera remis en classe pendant votre séance de TP au serveur INGInious - <span style='color:Red'> aucun document ou code ne sera accepté si envoyé par mail ou clé USB</span>**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
